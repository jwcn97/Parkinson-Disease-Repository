{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from scipy import signal\n",
    "from itertools import combinations, product\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import truncation\n",
    "import handle\n",
    "\n",
    "def timestat_acc(acc_t):\n",
    "    appended1, appended3 = [], []\n",
    "    Lx, Ly, Lz = [], [], []\n",
    "    \n",
    "    for t in acc_t:\n",
    "        # appending number of zero crossing\n",
    "        L1 = t['x-axis (g)'].values.tolist()\n",
    "        ar_x = np.array(L1)\n",
    "        zcross_xa = ((ar_x[:-1] * ar_x[1:]) < 0).sum()\n",
    "       \n",
    "        L2 = t['y-axis (g)'].values.tolist()\n",
    "        ar_y = np.array(L2)\n",
    "        zcross_ya = ((ar_y[:-1] * ar_y[1:]) < 0).sum()\n",
    "        \n",
    "        L3 = t['z-axis (g)'].values.tolist()\n",
    "        ar_z = np.array(L3)\n",
    "        zcross_za = ((ar_z[:-1] * ar_z[1:]) < 0).sum()\n",
    "        \n",
    "        Lx.append(ar_x)\n",
    "        Ly.append(ar_y)\n",
    "        Lz.append(ar_z)\n",
    "        appended1.append([zcross_xa, zcross_ya, zcross_za])\n",
    "        appended1.append([np.mean(L1), np.mean(L2), np.mean(L3)])\n",
    "        appended1.append([np.std(L1), np.std(L2), np.std(L3)])\n",
    "        \n",
    "        # appending number of mean crossing\n",
    "        xcrossings_a, ycrossings_a, zcrossings_a = [], [], []\n",
    "        for i in range(1, len(ar_x)):\n",
    "            if ar_x[i-1] < np.mean(L1) and ar_x[i] >= np.mean(L1):\n",
    "                xcrossings_a.append(i)\n",
    "            if ar_y[i-1] < np.mean(L2) and ar_y[i] >= np.mean(L2):\n",
    "                ycrossings_a.append(i)\n",
    "            if ar_z[i-1] < np.mean(L3) and ar_z[i] >= np.mean(L3):\n",
    "                zcrossings_a.append(i)\n",
    "                \n",
    "        appended1.append([len(xcrossings_a), len(ycrossings_a), len(zcrossings_a)])\n",
    "    \n",
    "    # calculating covariance\n",
    "    for i in range(len(Lx)):\n",
    "        # setting conditions to deducting corresponding mean values from the array of different tries\n",
    "        Lxi = Lx[i] - appended1[4*i+1][0]\n",
    "        Lyi = Ly[i] - appended1[4*i+1][1]\n",
    "        Lzi = Lz[i] - appended1[4*i+1][2]\n",
    "\n",
    "        # using combination to calculate SOP of values divided by the number of values\n",
    "        for i in combinations([Lxi,Lyi,Lzi],2):\n",
    "            for x,y in [i]:\n",
    "                appended3.append(sum(x*y)/(len(Lxi)-1))\n",
    "    \n",
    "    return appended1,appended3\n",
    "    \n",
    "def timestat_gyr(gyro_t):\n",
    "    appended2, appended4 = [], []\n",
    "    L_x, L_y, L_z = [], [], []\n",
    "    \n",
    "    for t in gyro_t:\n",
    "        L4 = t['x-axis (deg/s)'].values.tolist()\n",
    "        ar_x = np.array(L4)\n",
    "        zcross_xg = ((ar_x[:-1] * ar_x[1:]) < 0).sum()\n",
    "        \n",
    "        L5 = t['y-axis (deg/s)'].values.tolist()\n",
    "        ar_y = np.array(L5)\n",
    "        zcross_yg = ((ar_y[:-1] * ar_y[1:]) < 0).sum()\n",
    "        \n",
    "        L6 = t['z-axis (deg/s)'].values.tolist()\n",
    "        ar_z = np.array(L6)\n",
    "        zcross_zg = ((ar_z[:-1] * ar_z[1:]) < 0).sum()\n",
    "        \n",
    "        L_x.append(ar_x)\n",
    "        L_y.append(ar_y)\n",
    "        L_z.append(ar_z)\n",
    "        appended2.append([zcross_xg, zcross_yg, zcross_zg])\n",
    "        appended2.append([np.mean(L4), np.mean(L5), np.mean(L6)])\n",
    "        appended2.append([np.std(L4), np.std(L5), np.std(L6)])\n",
    "        \n",
    "        xcrossings_g, ycrossings_g, zcrossings_g = [], [], []\n",
    "        for i in range(1, len(ar_x)):\n",
    "            if ar_x[i-1] < np.mean(L4) and ar_x[i] >= np.mean(L4):\n",
    "                xcrossings_g.append(i)\n",
    "            if ar_y[i-1] < np.mean(L5) and ar_y[i] >= np.mean(L5):\n",
    "                ycrossings_g.append(i)\n",
    "            if ar_z[i-1] < np.mean(L6) and ar_z[i] >= np.mean(L6):\n",
    "                zcrossings_g.append(i)\n",
    "                \n",
    "        appended2.append([len(xcrossings_g), len(ycrossings_g), len(zcrossings_g)])\n",
    "        \n",
    "    for i in range(len(L_x)):\n",
    "        Lxi = L_x[i] - appended2[4*i+1][0]\n",
    "        Lyi = L_y[i] - appended2[4*i+1][1]\n",
    "        Lzi = L_z[i] - appended2[4*i+1][2]\n",
    "\n",
    "        for i in combinations([Lxi,Lyi,Lzi],2):\n",
    "            for x,y in [i]:\n",
    "                appended4.append(sum(x*y)/(len(Lxi)-1))\n",
    "\n",
    "    return appended2,appended4\n",
    "    \n",
    "def cal_magnitude(df, file, window):\n",
    "    # combine triaxial data into one singular magnitude data\n",
    "    if 'Accelerometer' in file:\n",
    "        df['x'] = df.loc[window-1:,'x-axis (g)']-df['x-axis (g)'].rolling(window).mean()[window-1:]\n",
    "        df['y'] = df.loc[window-1:,'y-axis (g)']-df['y-axis (g)'].rolling(window).mean()[window-1:]\n",
    "        df['z'] = df.loc[window-1:,'z-axis (g)']-df['z-axis (g)'].rolling(window).mean()[window-1:]\n",
    "        \n",
    "    elif 'Gyroscope' in file:\n",
    "        df['x'] = df['x-axis (deg/s)']\n",
    "        df['y'] = df['y-axis (deg/s)']\n",
    "        df['z'] = df['z-axis (deg/s)']\n",
    "        \n",
    "    df['resultant'] = df['x']**2 + df['y']**2 + df['z']**2\n",
    "    df['resultant'] = df['resultant'].apply(math.sqrt)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# MAIN FUNCTION\n",
    "def main(filepath='/', window=3, truncate=None, columns=None):\n",
    "    info = []\n",
    "    path = 'Patient003/' + filepath + '.csv'\n",
    "    file_name = path.split('/')[1]\n",
    "    title_main = file_name.split('_')[0]\n",
    "\n",
    "    # append user's requested metrics to the default metrics\n",
    "    columns = ['test','lvl','duration'] + columns\n",
    "\n",
    "    # extract dataframe\n",
    "    df_main = pd.read_csv(path)\n",
    "\n",
    "    # find moving average of triaxial data and combine them into one singular magnitude data\n",
    "    df_main = cal_magnitude(df_main, file_name, window)\n",
    "\n",
    "    acc, gyr = [], []\n",
    "    for test in range(3):\n",
    "        # include tries in title\n",
    "        title = (title_main.split('-')[0] + '-' + title_main.split('-')[1] + \n",
    "                 '-try' + str(test+1) + '-' + title_main.split('-')[2] + \n",
    "                 '-' + file_name.split('_')[4])\n",
    "\n",
    "        # truncate signal beyond start and end times\n",
    "        if (truncate is not None) and (truncate[filepath] is not None) and (truncate[filepath][test] != ()):\n",
    "            df = df_main.loc[(df_main['elapsed (s)'] <= truncate[filepath][test][1]) & \n",
    "                             (df_main['elapsed (s)'] >= truncate[filepath][test][0])]\n",
    "            df.reset_index(inplace=True)\n",
    "        else:\n",
    "            df = df_main\n",
    "\n",
    "        # find peaks and troughs in signal\n",
    "        time_p_main, p_plot_main, peaks_main = handle.find_peaks(title, df, 10)\n",
    "        time_t_main, t_plot_main, troughs_main = handle.find_troughs(title, df, 10)\n",
    "        \n",
    "        # find jitter peaks and troughs in signal\n",
    "        time_p_jitter, p_plot_jitter, peaks_jitter = handle.find_peaks(title, df, None)\n",
    "        time_t_jitter, t_plot_jitter, troughs_jitter = handle.find_troughs(title, df, None)\n",
    "        \n",
    "        # separate into different axis\n",
    "        axes = ['x', 'y', 'z', 'resultant']\n",
    "        time_p_j_main = []\n",
    "        p_plot_j_main = []\n",
    "        time_t_j_main = []\n",
    "        t_plot_j_main = []\n",
    "        \n",
    "        for i in range(len(axes)):\n",
    "            result_main = pd.concat([peaks_main[i], troughs_main[i]], axis=0, join='outer', ignore_index=False)\n",
    "            result_main = result_main.sort_values(by=['time'])\n",
    "            result = result_main.reset_index(drop=True)\n",
    "\n",
    "            result_main = pd.concat([peaks_jitter[i], troughs_jitter[i]], axis=0, join='outer', ignore_index=False)\n",
    "            result_main = result_main.sort_values(by=['time'])\n",
    "            result_main = result_main.reset_index(drop=True)\n",
    "            result_jitters = handle.get_jitters(result_main)\n",
    "\n",
    "            time_p_j_main.append(list(result_jitters[result_jitters['peaks'] > 0]['time']))\n",
    "            p_plot_j_main.append(list(result_jitters[result_jitters['peaks'] > 0]['peaks']))\n",
    "            time_t_j_main.append(list(result_jitters[result_jitters['peaks'] < 0]['time']))\n",
    "            t_plot_j_main.append(list(result_jitters[result_jitters['peaks'] < 0]['peaks']))\n",
    "\n",
    "            # plot altered peak and troughs with original signal\n",
    "            ax1=df.plot(x='elapsed (s)', y=[axes[i]], figsize=(16,7), title=title)\n",
    "            result.plot.scatter(x='time', y='peaks', ax=ax1, s=140, c='r')\n",
    "            result_jitters.plot.scatter(x='time', y='peaks', ax=ax1, s=70, c='b')\n",
    "            ax1.grid(True)\n",
    "            ax1.set_xlabel('time (s)')\n",
    "            ax1.set_ylabel('amplitude')\n",
    "            plt.show()\n",
    "\n",
    "        magunit = 'm s^-2' if 'Accelerometer' in file_name else 'degree s^-1'\n",
    "\n",
    "        obj = pd.DataFrame({\n",
    "            'author': ['JW'] * 22,\n",
    "            'trial': [test] * 22,\n",
    "            'level': [int(title_main.split('-')[1][-1])] * 22,\n",
    "            'sensor_mode': [file_name.split('_')[4]] * 22,\n",
    "            'sensor_location': [title_main.split('-')[2]] * 22,\n",
    "            'metric_name': ['test', 'duration',\n",
    "                          'peak actions', 'peak height', 'peak height std', 'peak width', 'peak width std',\n",
    "                          'peak actions_j', 'peak height_j', 'peak height_j std', 'peak width_j', 'peak width_j std',\n",
    "                          'trough actions', 'trough height', 'trough height std', 'trough width', 'trough width std',\n",
    "                          'trough actions_j', 'trough height_j', 'trough height_j std', 'trough width_j', 'trough width_j std'],\n",
    "            'metric_description': ['test', 'duration',\n",
    "                                   'number of main peaks', 'average height of main peaks', 'standard deviation of height of main peaks', 'average width of main peaks', 'standard deviation of width of main peaks',\n",
    "                                   'number of jitter peaks', 'average height of jitter peaks', 'standard deviation of height of jitter peaks', 'average width of jitter peaks', 'standard deviation of width of jitter peaks',\n",
    "                                   'number of main troughs', 'average height of main troughs', 'standard deviation of height of main troughs', 'average width of main troughs', 'standard deviation of width of main troughs',\n",
    "                                   'number of jitter troughs', 'average height of jitter troughs', 'standard deviation of height of jitter troughs', 'average width of jitter troughs', 'standard deviation of width of jitter troughs'],\n",
    "            'x': [title_main.split('-')[0], df.loc[len(df)-1,'elapsed (s)']-df.loc[0,'elapsed (s)'],\n",
    "                  len(time_p_main[0]), np.mean(p_plot_main[0]), np.std(p_plot_main[0]), np.mean(np.diff(time_p_main[0])), np.std(np.diff(time_p_main[0])),\n",
    "                  len(time_p_j_main[0]), np.mean(p_plot_j_main[0]), np.std(p_plot_j_main[0]), np.mean(np.diff(time_p_j_main[0])), np.std(np.diff(time_p_j_main[0])),\n",
    "                  len(time_t_main[0]), np.mean(t_plot_main[0]), np.std(t_plot_main[0]), np.mean(np.diff(time_t_main[0])), np.std(np.diff(time_t_main[0])),\n",
    "                  len(time_t_j_main[0]), np.mean(t_plot_j_main[0]), np.std(t_plot_j_main[0]), np.mean(np.diff(time_t_j_main[0])), np.std(np.diff(time_t_j_main[0]))],\n",
    "            'y': [title_main.split('-')[0], df.loc[len(df)-1,'elapsed (s)']-df.loc[0,'elapsed (s)'],\n",
    "                  len(time_p_main[1]), np.mean(p_plot_main[1]), np.std(p_plot_main[1]), np.mean(np.diff(time_p_main[1])), np.std(np.diff(time_p_main[1])),\n",
    "                  len(time_p_j_main[1]), np.mean(p_plot_j_main[1]), np.std(p_plot_j_main[1]), np.mean(np.diff(time_p_j_main[1])), np.std(np.diff(time_p_j_main[1])),\n",
    "                  len(time_t_main[1]), np.mean(t_plot_main[1]), np.std(t_plot_main[1]), np.mean(np.diff(time_t_main[1])), np.std(np.diff(time_t_main[1])),\n",
    "                  len(time_t_j_main[1]), np.mean(t_plot_j_main[1]), np.std(t_plot_j_main[1]), np.mean(np.diff(time_t_j_main[1])), np.std(np.diff(time_t_j_main[1]))],\n",
    "            'z': [title_main.split('-')[0], df.loc[len(df)-1,'elapsed (s)']-df.loc[0,'elapsed (s)'],\n",
    "                  len(time_p_main[2]), np.mean(p_plot_main[2]), np.std(p_plot_main[2]), np.mean(np.diff(time_p_main[2])), np.std(np.diff(time_p_main[2])),\n",
    "                  len(time_p_j_main[2]), np.mean(p_plot_j_main[2]), np.std(p_plot_j_main[2]), np.mean(np.diff(time_p_j_main[2])), np.std(np.diff(time_p_j_main[2])),\n",
    "                  len(time_t_main[2]), np.mean(t_plot_main[2]), np.std(t_plot_main[2]), np.mean(np.diff(time_t_main[2])), np.std(np.diff(time_t_main[2])),\n",
    "                  len(time_t_j_main[2]), np.mean(t_plot_j_main[2]), np.std(t_plot_j_main[2]), np.mean(np.diff(time_t_j_main[2])), np.std(np.diff(time_t_j_main[2]))],\n",
    "            'resultant': [title_main.split('-')[0], df.loc[len(df)-1,'elapsed (s)']-df.loc[0,'elapsed (s)'],\n",
    "                          len(time_p_main[3]), np.mean(p_plot_main[3]), np.std(p_plot_main[3]), np.mean(np.diff(time_p_main[3])), np.std(np.diff(time_p_main[3])),\n",
    "                          len(time_p_j_main[3]), np.mean(p_plot_j_main[3]), np.std(p_plot_j_main[3]), np.mean(np.diff(time_p_j_main[3])), np.std(np.diff(time_p_j_main[3]))] +\n",
    "                          (['-'] * 10),\n",
    "            'x_unit': ['-','s'] + ['-', magunit, '-', 's', '-'] * 4,\n",
    "            'y_unit': ['-','s'] + ['-', magunit, '-', 's', '-'] * 4,\n",
    "            'z_unit': ['-','s'] + ['-', magunit, '-', 's', '-'] * 4,\n",
    "            'resultant_unit': ['-','s'] + ['-', magunit, '-', 's', '-'] * 4\n",
    "        })\n",
    "\n",
    "        info += eval(obj[obj['metric_name'].isin(columns)].to_json(orient='records'))\n",
    "\n",
    "        # for zero crossing rate, mean crossing rate and covariance\n",
    "        if 'Accelerometer' in file_name: acc.append(df)\n",
    "        else:                            gyr.append(df)\n",
    "\n",
    "    # for zero crossing rate, mean crossing rate and covariance\n",
    "    if 'Accelerometer' in file_name: appended,cov = timestat_acc(acc)\n",
    "    else:                            appended,cov = timestat_gyr(gyr)\n",
    "\n",
    "    # append zero crossing rate, mean crossing rate and covariance data\n",
    "    for i in range(3):\n",
    "        obj2 = pd.DataFrame({\n",
    "            'author': ['CY'] * 5,\n",
    "            'trial': [i] * 5,\n",
    "            'level': [int(title_main.split('-')[1][-1])] * 5,\n",
    "            'sensor_mode': [file_name.split('_')[4]] * 5,\n",
    "            'sensor_location': [title_main.split('-')[2]] * 5,\n",
    "            'metric_name': ['zcr', 'mcr', 'xy', 'xz', 'yz'],\n",
    "            'metric_description': ['zero crossing rate', 'mean crossing rate',\n",
    "                                   'xy covariance', 'xz covariance', 'yz covariance'],\n",
    "            'x': [appended[i*4][0]/timer[i], appended[i*4+3][0], cov[i*3+0], cov[i*3+1], cov[i*3+2]],\n",
    "            'y': [appended[i*4][1]/timer[i], appended[i*4+3][1], cov[i*3+0], cov[i*3+1], cov[i*3+2]],\n",
    "            'z': [appended[i*4][2]/timer[i], appended[i*4+3][2], cov[i*3+0], cov[i*3+1], cov[i*3+2]],\n",
    "            'resultant': ['-'] * 5,\n",
    "            'x_unit': ['-'] * 5,\n",
    "            'y_unit': ['-'] * 5,\n",
    "            'z_unit': ['-'] * 5,\n",
    "            'resultant_unit': ['-'] * 5\n",
    "        })\n",
    "        \n",
    "        # append stats into dataframe if it is requested\n",
    "        info += eval(obj2[obj2['metric_name'].isin(columns)].to_json(orient='records'))\n",
    "\n",
    "    return info\n",
    "\n",
    "cols = ['peak actions', 'peak height', 'peak height_j std', 'peak width_j', 'trough width_j std',\n",
    "        'zcr', 'mcr', 'xy', 'xz', 'yz']\n",
    "\n",
    "info = main(filepath=input('Please Insert File Name (without .csv): '), truncate=truncation.patient3, columns=cols)\n",
    "print(json.dumps(info, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
