{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELEC0118: Fourth Year MEng Projects \n",
    "# Sensor Arrays for Movement Sensing\n",
    "## Members: Ken Yew Piong, Ka Shing Liong, Jing Wei Chan, Chin Yang Tan\n",
    "\n",
    "# 1. Introduction\n",
    "Wearable technology has considerable potential in healthcare. This project is concerned with the use of motion sensors such as accelerometers in the self-management of symptoms of a neurological condition called Parkinson’s Disease (PD) which has multiple motor symptoms including tremor and slowness of movement.\n",
    "\n",
    "Motion sensors are already widely used in fitness monitoring, but the raw data is not readily available from commercial systems. For this project, data will be streamed and stored from the sensor array device containing accelerometers and gyroscopes that is worn by the patient. The signals from these sensors are processed to extract features which are characteristic of particular movements. The challenge is to identify the signals for the particular movements from other intended movements and study the variation of these movements during the day.\n",
    "\n",
    "## 1.1 Motivation\n",
    "This notebook will investigate signal processing algorithms to extract the relevant movement data and suggest parameters that can be clearly provided to clinicians to quantify the variation in relevant movement during a 24-hour period.\n",
    "\n",
    "We will first visualise the given data through many plotting methods such as boxplots, scatter plots, heatmaps and histograms to gain a better understanding on the correlation of each feature. We will also be using dimensionality reduction methods to visualise the data more elegantly and capture an overall understanding of the trend of the data. **IN PROGRESS - PENDING ACTUAL DATA** \n",
    "\n",
    "We will then focus on running the following machine learning models for this case and then compute their corresponding performance scores to determine the most effective and feasible model for this application. \n",
    "1. Linear Logistic Regression\n",
    "2. K-Nearest Neighbours Classifier\n",
    "3. Linear Support Vector Machine\n",
    "4. Kernel RBF Support Vector Machine\n",
    "5. Adaptive Gradient Boosting\n",
    "6. Random Forest\n",
    "7. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading, Wrangling and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the dataframe (cols x rows): (195, 24)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\"\n",
    "dataset = pd.read_csv(url)\n",
    "df = dataset.copy() # df - dataframe\n",
    "print(\"The dimensions of the dataframe (cols x rows):\", df.shape) # (195, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MDVP:Fo(Hz)</th>\n",
       "      <th>MDVP:Fhi(Hz)</th>\n",
       "      <th>MDVP:Flo(Hz)</th>\n",
       "      <th>MDVP:Jitter(%)</th>\n",
       "      <th>MDVP:Jitter(Abs)</th>\n",
       "      <th>MDVP:RAP</th>\n",
       "      <th>MDVP:PPQ</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>MDVP:Shimmer</th>\n",
       "      <th>MDVP:Shimmer(dB)</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>status</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>spread1</th>\n",
       "      <th>spread2</th>\n",
       "      <th>D2</th>\n",
       "      <th>PPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>154.228641</td>\n",
       "      <td>197.104918</td>\n",
       "      <td>116.324631</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.003446</td>\n",
       "      <td>0.009920</td>\n",
       "      <td>0.029709</td>\n",
       "      <td>0.282251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046993</td>\n",
       "      <td>0.024847</td>\n",
       "      <td>21.885974</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.498536</td>\n",
       "      <td>0.718099</td>\n",
       "      <td>-5.684397</td>\n",
       "      <td>0.226510</td>\n",
       "      <td>2.381826</td>\n",
       "      <td>0.206552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>41.390065</td>\n",
       "      <td>91.491548</td>\n",
       "      <td>43.521413</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.008903</td>\n",
       "      <td>0.018857</td>\n",
       "      <td>0.194877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030459</td>\n",
       "      <td>0.040418</td>\n",
       "      <td>4.425764</td>\n",
       "      <td>0.431878</td>\n",
       "      <td>0.103942</td>\n",
       "      <td>0.055336</td>\n",
       "      <td>1.090208</td>\n",
       "      <td>0.083406</td>\n",
       "      <td>0.382799</td>\n",
       "      <td>0.090119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>88.333000</td>\n",
       "      <td>102.145000</td>\n",
       "      <td>65.476000</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>8.441000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256570</td>\n",
       "      <td>0.574282</td>\n",
       "      <td>-7.964984</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>1.423287</td>\n",
       "      <td>0.044539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>117.572000</td>\n",
       "      <td>134.862500</td>\n",
       "      <td>84.291000</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>0.016505</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024735</td>\n",
       "      <td>0.005925</td>\n",
       "      <td>19.198000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421306</td>\n",
       "      <td>0.674758</td>\n",
       "      <td>-6.450096</td>\n",
       "      <td>0.174351</td>\n",
       "      <td>2.099125</td>\n",
       "      <td>0.137451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>148.790000</td>\n",
       "      <td>175.829000</td>\n",
       "      <td>104.315000</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>0.022970</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038360</td>\n",
       "      <td>0.011660</td>\n",
       "      <td>22.085000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.495954</td>\n",
       "      <td>0.722254</td>\n",
       "      <td>-5.720868</td>\n",
       "      <td>0.218885</td>\n",
       "      <td>2.361532</td>\n",
       "      <td>0.194052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>182.769000</td>\n",
       "      <td>224.205500</td>\n",
       "      <td>140.018500</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.011505</td>\n",
       "      <td>0.037885</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060795</td>\n",
       "      <td>0.025640</td>\n",
       "      <td>25.075500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587562</td>\n",
       "      <td>0.761881</td>\n",
       "      <td>-5.046192</td>\n",
       "      <td>0.279234</td>\n",
       "      <td>2.636456</td>\n",
       "      <td>0.252980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>260.105000</td>\n",
       "      <td>592.030000</td>\n",
       "      <td>239.170000</td>\n",
       "      <td>0.033160</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.021440</td>\n",
       "      <td>0.019580</td>\n",
       "      <td>0.064330</td>\n",
       "      <td>0.119080</td>\n",
       "      <td>1.302000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169420</td>\n",
       "      <td>0.314820</td>\n",
       "      <td>33.047000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.685151</td>\n",
       "      <td>0.825288</td>\n",
       "      <td>-2.434031</td>\n",
       "      <td>0.450493</td>\n",
       "      <td>3.671155</td>\n",
       "      <td>0.527367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
       "count   195.000000    195.000000    195.000000      195.000000   \n",
       "mean    154.228641    197.104918    116.324631        0.006220   \n",
       "std      41.390065     91.491548     43.521413        0.004848   \n",
       "min      88.333000    102.145000     65.476000        0.001680   \n",
       "25%     117.572000    134.862500     84.291000        0.003460   \n",
       "50%     148.790000    175.829000    104.315000        0.004940   \n",
       "75%     182.769000    224.205500    140.018500        0.007365   \n",
       "max     260.105000    592.030000    239.170000        0.033160   \n",
       "\n",
       "       MDVP:Jitter(Abs)    MDVP:RAP    MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  \\\n",
       "count        195.000000  195.000000  195.000000  195.000000    195.000000   \n",
       "mean           0.000044    0.003306    0.003446    0.009920      0.029709   \n",
       "std            0.000035    0.002968    0.002759    0.008903      0.018857   \n",
       "min            0.000007    0.000680    0.000920    0.002040      0.009540   \n",
       "25%            0.000020    0.001660    0.001860    0.004985      0.016505   \n",
       "50%            0.000030    0.002500    0.002690    0.007490      0.022970   \n",
       "75%            0.000060    0.003835    0.003955    0.011505      0.037885   \n",
       "max            0.000260    0.021440    0.019580    0.064330      0.119080   \n",
       "\n",
       "       MDVP:Shimmer(dB)  ...  Shimmer:DDA         NHR         HNR      status  \\\n",
       "count        195.000000  ...   195.000000  195.000000  195.000000  195.000000   \n",
       "mean           0.282251  ...     0.046993    0.024847   21.885974    0.753846   \n",
       "std            0.194877  ...     0.030459    0.040418    4.425764    0.431878   \n",
       "min            0.085000  ...     0.013640    0.000650    8.441000    0.000000   \n",
       "25%            0.148500  ...     0.024735    0.005925   19.198000    1.000000   \n",
       "50%            0.221000  ...     0.038360    0.011660   22.085000    1.000000   \n",
       "75%            0.350000  ...     0.060795    0.025640   25.075500    1.000000   \n",
       "max            1.302000  ...     0.169420    0.314820   33.047000    1.000000   \n",
       "\n",
       "             RPDE         DFA     spread1     spread2          D2         PPE  \n",
       "count  195.000000  195.000000  195.000000  195.000000  195.000000  195.000000  \n",
       "mean     0.498536    0.718099   -5.684397    0.226510    2.381826    0.206552  \n",
       "std      0.103942    0.055336    1.090208    0.083406    0.382799    0.090119  \n",
       "min      0.256570    0.574282   -7.964984    0.006274    1.423287    0.044539  \n",
       "25%      0.421306    0.674758   -6.450096    0.174351    2.099125    0.137451  \n",
       "50%      0.495954    0.722254   -5.720868    0.218885    2.361532    0.194052  \n",
       "75%      0.587562    0.761881   -5.046192    0.279234    2.636456    0.252980  \n",
       "max      0.685151    0.825288   -2.434031    0.450493    3.671155    0.527367  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dataframe Attribute Information\n",
    "\n",
    "### Matrix column entries (attributes):\n",
    "**name**: ASCII subject name and recording number\n",
    "\n",
    "**MDVP:Fo(Hz)**: Average vocal fundamental frequency\n",
    "\n",
    "**MDVP:Fhi(Hz)**: Maximum vocal fundamental frequency\n",
    "\n",
    "**MDVP:Flo(Hz)**: Minimum vocal fundamental frequency\n",
    "\n",
    "**MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP**: Several measures of variation in fundamental frequency\n",
    "\n",
    "**MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA**: Several measures of variation in amplitude\n",
    "\n",
    "**NHR,HNR**: Two measures of ratio of noise to tonal components in the voice\n",
    "\n",
    "**status**: Health status of the subject (one) - Parkinson's, (zero) - healthy\n",
    "\n",
    "**RPDE,D2**: Two nonlinear dynamical complexity measures\n",
    "\n",
    "**DFA**: Signal fractal scaling exponent\n",
    "\n",
    "**spread1,spread2,PPE**: Three nonlinear measures of fundamental frequency variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Restructuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MDVP:Fo(Hz)</th>\n",
       "      <th>MDVP:Fhi(Hz)</th>\n",
       "      <th>MDVP:Flo(Hz)</th>\n",
       "      <th>MDVP:Jitter(%)</th>\n",
       "      <th>MDVP:Jitter(Abs)</th>\n",
       "      <th>MDVP:RAP</th>\n",
       "      <th>MDVP:PPQ</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>MDVP:Shimmer</th>\n",
       "      <th>MDVP:Shimmer(dB)</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>spread1</th>\n",
       "      <th>spread2</th>\n",
       "      <th>D2</th>\n",
       "      <th>PPE</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>119.992</td>\n",
       "      <td>157.302</td>\n",
       "      <td>74.997</td>\n",
       "      <td>0.00784</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00370</td>\n",
       "      <td>0.00554</td>\n",
       "      <td>0.01109</td>\n",
       "      <td>0.04374</td>\n",
       "      <td>0.426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06545</td>\n",
       "      <td>0.02211</td>\n",
       "      <td>21.033</td>\n",
       "      <td>0.414783</td>\n",
       "      <td>0.815285</td>\n",
       "      <td>-4.813031</td>\n",
       "      <td>0.266482</td>\n",
       "      <td>2.301442</td>\n",
       "      <td>0.284654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>122.400</td>\n",
       "      <td>148.650</td>\n",
       "      <td>113.819</td>\n",
       "      <td>0.00968</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.00696</td>\n",
       "      <td>0.01394</td>\n",
       "      <td>0.06134</td>\n",
       "      <td>0.626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09403</td>\n",
       "      <td>0.01929</td>\n",
       "      <td>19.085</td>\n",
       "      <td>0.458359</td>\n",
       "      <td>0.819521</td>\n",
       "      <td>-4.075192</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>2.486855</td>\n",
       "      <td>0.368674</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>116.682</td>\n",
       "      <td>131.111</td>\n",
       "      <td>111.555</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00544</td>\n",
       "      <td>0.00781</td>\n",
       "      <td>0.01633</td>\n",
       "      <td>0.05233</td>\n",
       "      <td>0.482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08270</td>\n",
       "      <td>0.01309</td>\n",
       "      <td>20.651</td>\n",
       "      <td>0.429895</td>\n",
       "      <td>0.825288</td>\n",
       "      <td>-4.443179</td>\n",
       "      <td>0.311173</td>\n",
       "      <td>2.342259</td>\n",
       "      <td>0.332634</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>116.676</td>\n",
       "      <td>137.871</td>\n",
       "      <td>111.366</td>\n",
       "      <td>0.00997</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00502</td>\n",
       "      <td>0.00698</td>\n",
       "      <td>0.01505</td>\n",
       "      <td>0.05492</td>\n",
       "      <td>0.517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08771</td>\n",
       "      <td>0.01353</td>\n",
       "      <td>20.644</td>\n",
       "      <td>0.434969</td>\n",
       "      <td>0.819235</td>\n",
       "      <td>-4.117501</td>\n",
       "      <td>0.334147</td>\n",
       "      <td>2.405554</td>\n",
       "      <td>0.368975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>116.014</td>\n",
       "      <td>141.781</td>\n",
       "      <td>110.655</td>\n",
       "      <td>0.01284</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00655</td>\n",
       "      <td>0.00908</td>\n",
       "      <td>0.01966</td>\n",
       "      <td>0.06425</td>\n",
       "      <td>0.584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.01767</td>\n",
       "      <td>19.649</td>\n",
       "      <td>0.417356</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>-3.747787</td>\n",
       "      <td>0.234513</td>\n",
       "      <td>2.332180</td>\n",
       "      <td>0.410335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>174.188</td>\n",
       "      <td>230.978</td>\n",
       "      <td>94.261</td>\n",
       "      <td>0.00459</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00263</td>\n",
       "      <td>0.00259</td>\n",
       "      <td>0.00790</td>\n",
       "      <td>0.04087</td>\n",
       "      <td>0.405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07008</td>\n",
       "      <td>0.02764</td>\n",
       "      <td>19.517</td>\n",
       "      <td>0.448439</td>\n",
       "      <td>0.657899</td>\n",
       "      <td>-6.538586</td>\n",
       "      <td>0.121952</td>\n",
       "      <td>2.657476</td>\n",
       "      <td>0.133050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>209.516</td>\n",
       "      <td>253.017</td>\n",
       "      <td>89.488</td>\n",
       "      <td>0.00564</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00331</td>\n",
       "      <td>0.00292</td>\n",
       "      <td>0.00994</td>\n",
       "      <td>0.02751</td>\n",
       "      <td>0.263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04812</td>\n",
       "      <td>0.01810</td>\n",
       "      <td>19.147</td>\n",
       "      <td>0.431674</td>\n",
       "      <td>0.683244</td>\n",
       "      <td>-6.195325</td>\n",
       "      <td>0.129303</td>\n",
       "      <td>2.784312</td>\n",
       "      <td>0.168895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>174.688</td>\n",
       "      <td>240.005</td>\n",
       "      <td>74.287</td>\n",
       "      <td>0.01360</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00624</td>\n",
       "      <td>0.00564</td>\n",
       "      <td>0.01873</td>\n",
       "      <td>0.02308</td>\n",
       "      <td>0.256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03804</td>\n",
       "      <td>0.10715</td>\n",
       "      <td>17.883</td>\n",
       "      <td>0.407567</td>\n",
       "      <td>0.655683</td>\n",
       "      <td>-6.787197</td>\n",
       "      <td>0.158453</td>\n",
       "      <td>2.679772</td>\n",
       "      <td>0.131728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>198.764</td>\n",
       "      <td>396.961</td>\n",
       "      <td>74.904</td>\n",
       "      <td>0.00740</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00370</td>\n",
       "      <td>0.00390</td>\n",
       "      <td>0.01109</td>\n",
       "      <td>0.02296</td>\n",
       "      <td>0.241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03794</td>\n",
       "      <td>0.07223</td>\n",
       "      <td>19.020</td>\n",
       "      <td>0.451221</td>\n",
       "      <td>0.643956</td>\n",
       "      <td>-6.744577</td>\n",
       "      <td>0.207454</td>\n",
       "      <td>2.138608</td>\n",
       "      <td>0.123306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>214.289</td>\n",
       "      <td>260.277</td>\n",
       "      <td>77.973</td>\n",
       "      <td>0.00567</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00295</td>\n",
       "      <td>0.00317</td>\n",
       "      <td>0.00885</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>0.190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03078</td>\n",
       "      <td>0.04398</td>\n",
       "      <td>21.209</td>\n",
       "      <td>0.462803</td>\n",
       "      <td>0.664357</td>\n",
       "      <td>-5.724056</td>\n",
       "      <td>0.190667</td>\n",
       "      <td>2.555477</td>\n",
       "      <td>0.148569</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
       "0        119.992       157.302        74.997         0.00784   \n",
       "1        122.400       148.650       113.819         0.00968   \n",
       "2        116.682       131.111       111.555         0.01050   \n",
       "3        116.676       137.871       111.366         0.00997   \n",
       "4        116.014       141.781       110.655         0.01284   \n",
       "..           ...           ...           ...             ...   \n",
       "190      174.188       230.978        94.261         0.00459   \n",
       "191      209.516       253.017        89.488         0.00564   \n",
       "192      174.688       240.005        74.287         0.01360   \n",
       "193      198.764       396.961        74.904         0.00740   \n",
       "194      214.289       260.277        77.973         0.00567   \n",
       "\n",
       "     MDVP:Jitter(Abs)  MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  \\\n",
       "0             0.00007   0.00370   0.00554     0.01109       0.04374   \n",
       "1             0.00008   0.00465   0.00696     0.01394       0.06134   \n",
       "2             0.00009   0.00544   0.00781     0.01633       0.05233   \n",
       "3             0.00009   0.00502   0.00698     0.01505       0.05492   \n",
       "4             0.00011   0.00655   0.00908     0.01966       0.06425   \n",
       "..                ...       ...       ...         ...           ...   \n",
       "190           0.00003   0.00263   0.00259     0.00790       0.04087   \n",
       "191           0.00003   0.00331   0.00292     0.00994       0.02751   \n",
       "192           0.00008   0.00624   0.00564     0.01873       0.02308   \n",
       "193           0.00004   0.00370   0.00390     0.01109       0.02296   \n",
       "194           0.00003   0.00295   0.00317     0.00885       0.01884   \n",
       "\n",
       "     MDVP:Shimmer(dB)  ...  Shimmer:DDA      NHR     HNR      RPDE       DFA  \\\n",
       "0               0.426  ...      0.06545  0.02211  21.033  0.414783  0.815285   \n",
       "1               0.626  ...      0.09403  0.01929  19.085  0.458359  0.819521   \n",
       "2               0.482  ...      0.08270  0.01309  20.651  0.429895  0.825288   \n",
       "3               0.517  ...      0.08771  0.01353  20.644  0.434969  0.819235   \n",
       "4               0.584  ...      0.10470  0.01767  19.649  0.417356  0.823484   \n",
       "..                ...  ...          ...      ...     ...       ...       ...   \n",
       "190             0.405  ...      0.07008  0.02764  19.517  0.448439  0.657899   \n",
       "191             0.263  ...      0.04812  0.01810  19.147  0.431674  0.683244   \n",
       "192             0.256  ...      0.03804  0.10715  17.883  0.407567  0.655683   \n",
       "193             0.241  ...      0.03794  0.07223  19.020  0.451221  0.643956   \n",
       "194             0.190  ...      0.03078  0.04398  21.209  0.462803  0.664357   \n",
       "\n",
       "      spread1   spread2        D2       PPE  result  \n",
       "0   -4.813031  0.266482  2.301442  0.284654       1  \n",
       "1   -4.075192  0.335590  2.486855  0.368674       1  \n",
       "2   -4.443179  0.311173  2.342259  0.332634       1  \n",
       "3   -4.117501  0.334147  2.405554  0.368975       1  \n",
       "4   -3.747787  0.234513  2.332180  0.410335       1  \n",
       "..        ...       ...       ...       ...     ...  \n",
       "190 -6.538586  0.121952  2.657476  0.133050       0  \n",
       "191 -6.195325  0.129303  2.784312  0.168895       0  \n",
       "192 -6.787197  0.158453  2.679772  0.131728       0  \n",
       "193 -6.744577  0.207454  2.138608  0.123306       0  \n",
       "194 -5.724056  0.190667  2.555477  0.148569       0  \n",
       "\n",
       "[195 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"result\"]=df[\"status\"] # duplicate status data and store to result\n",
    "df.drop([\"name\",\"status\"],axis=1,inplace=True)# dropping columns (status now replaced with result)\n",
    "df.shape\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>154.228641</td>\n",
       "      <td>197.104918</td>\n",
       "      <td>116.324631</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.003446</td>\n",
       "      <td>0.009920</td>\n",
       "      <td>0.029709</td>\n",
       "      <td>0.282251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046993</td>\n",
       "      <td>0.024847</td>\n",
       "      <td>21.885974</td>\n",
       "      <td>0.498536</td>\n",
       "      <td>0.718099</td>\n",
       "      <td>-5.684397</td>\n",
       "      <td>0.226510</td>\n",
       "      <td>2.381826</td>\n",
       "      <td>0.206552</td>\n",
       "      <td>0.753846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>41.390065</td>\n",
       "      <td>91.491548</td>\n",
       "      <td>43.521413</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.008903</td>\n",
       "      <td>0.018857</td>\n",
       "      <td>0.194877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030459</td>\n",
       "      <td>0.040418</td>\n",
       "      <td>4.425764</td>\n",
       "      <td>0.103942</td>\n",
       "      <td>0.055336</td>\n",
       "      <td>1.090208</td>\n",
       "      <td>0.083406</td>\n",
       "      <td>0.382799</td>\n",
       "      <td>0.090119</td>\n",
       "      <td>0.431878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>88.333000</td>\n",
       "      <td>102.145000</td>\n",
       "      <td>65.476000</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>8.441000</td>\n",
       "      <td>0.256570</td>\n",
       "      <td>0.574282</td>\n",
       "      <td>-7.964984</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>1.423287</td>\n",
       "      <td>0.044539</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>117.572000</td>\n",
       "      <td>134.862500</td>\n",
       "      <td>84.291000</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>0.016505</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024735</td>\n",
       "      <td>0.005925</td>\n",
       "      <td>19.198000</td>\n",
       "      <td>0.421306</td>\n",
       "      <td>0.674758</td>\n",
       "      <td>-6.450096</td>\n",
       "      <td>0.174351</td>\n",
       "      <td>2.099125</td>\n",
       "      <td>0.137451</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>148.790000</td>\n",
       "      <td>175.829000</td>\n",
       "      <td>104.315000</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>0.022970</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038360</td>\n",
       "      <td>0.011660</td>\n",
       "      <td>22.085000</td>\n",
       "      <td>0.495954</td>\n",
       "      <td>0.722254</td>\n",
       "      <td>-5.720868</td>\n",
       "      <td>0.218885</td>\n",
       "      <td>2.361532</td>\n",
       "      <td>0.194052</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>182.769000</td>\n",
       "      <td>224.205500</td>\n",
       "      <td>140.018500</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.011505</td>\n",
       "      <td>0.037885</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060795</td>\n",
       "      <td>0.025640</td>\n",
       "      <td>25.075500</td>\n",
       "      <td>0.587562</td>\n",
       "      <td>0.761881</td>\n",
       "      <td>-5.046192</td>\n",
       "      <td>0.279234</td>\n",
       "      <td>2.636456</td>\n",
       "      <td>0.252980</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>260.105000</td>\n",
       "      <td>592.030000</td>\n",
       "      <td>239.170000</td>\n",
       "      <td>0.033160</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.021440</td>\n",
       "      <td>0.019580</td>\n",
       "      <td>0.064330</td>\n",
       "      <td>0.119080</td>\n",
       "      <td>1.302000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169420</td>\n",
       "      <td>0.314820</td>\n",
       "      <td>33.047000</td>\n",
       "      <td>0.685151</td>\n",
       "      <td>0.825288</td>\n",
       "      <td>-2.434031</td>\n",
       "      <td>0.450493</td>\n",
       "      <td>3.671155</td>\n",
       "      <td>0.527367</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  195.000000  195.000000  195.000000  195.000000  195.000000  195.000000   \n",
       "mean   154.228641  197.104918  116.324631    0.006220    0.000044    0.003306   \n",
       "std     41.390065   91.491548   43.521413    0.004848    0.000035    0.002968   \n",
       "min     88.333000  102.145000   65.476000    0.001680    0.000007    0.000680   \n",
       "25%    117.572000  134.862500   84.291000    0.003460    0.000020    0.001660   \n",
       "50%    148.790000  175.829000  104.315000    0.004940    0.000030    0.002500   \n",
       "75%    182.769000  224.205500  140.018500    0.007365    0.000060    0.003835   \n",
       "max    260.105000  592.030000  239.170000    0.033160    0.000260    0.021440   \n",
       "\n",
       "               6           7           8           9   ...          13  \\\n",
       "count  195.000000  195.000000  195.000000  195.000000  ...  195.000000   \n",
       "mean     0.003446    0.009920    0.029709    0.282251  ...    0.046993   \n",
       "std      0.002759    0.008903    0.018857    0.194877  ...    0.030459   \n",
       "min      0.000920    0.002040    0.009540    0.085000  ...    0.013640   \n",
       "25%      0.001860    0.004985    0.016505    0.148500  ...    0.024735   \n",
       "50%      0.002690    0.007490    0.022970    0.221000  ...    0.038360   \n",
       "75%      0.003955    0.011505    0.037885    0.350000  ...    0.060795   \n",
       "max      0.019580    0.064330    0.119080    1.302000  ...    0.169420   \n",
       "\n",
       "               14          15          16          17          18          19  \\\n",
       "count  195.000000  195.000000  195.000000  195.000000  195.000000  195.000000   \n",
       "mean     0.024847   21.885974    0.498536    0.718099   -5.684397    0.226510   \n",
       "std      0.040418    4.425764    0.103942    0.055336    1.090208    0.083406   \n",
       "min      0.000650    8.441000    0.256570    0.574282   -7.964984    0.006274   \n",
       "25%      0.005925   19.198000    0.421306    0.674758   -6.450096    0.174351   \n",
       "50%      0.011660   22.085000    0.495954    0.722254   -5.720868    0.218885   \n",
       "75%      0.025640   25.075500    0.587562    0.761881   -5.046192    0.279234   \n",
       "max      0.314820   33.047000    0.685151    0.825288   -2.434031    0.450493   \n",
       "\n",
       "               20          21          22  \n",
       "count  195.000000  195.000000  195.000000  \n",
       "mean     2.381826    0.206552    0.753846  \n",
       "std      0.382799    0.090119    0.431878  \n",
       "min      1.423287    0.044539    0.000000  \n",
       "25%      2.099125    0.137451    1.000000  \n",
       "50%      2.361532    0.194052    1.000000  \n",
       "75%      2.636456    0.252980    1.000000  \n",
       "max      3.671155    0.527367    1.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [i for i in range(23)] # renaming columns to numbers 0:23\n",
    "df.describe() # check if there are no missing values after the data restructuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Restructured Dataframe Attribute Information\n",
    "\n",
    "### Matrix column entries (attributes):\n",
    "**0**: name - ASCII subject name and recording number\n",
    "\n",
    "**1**: MDVP:Fo(Hz) - Average vocal fundamental frequency\n",
    "\n",
    "**2**: MDVP:Fhi(Hz) - Maximum vocal fundamental frequency\n",
    "\n",
    "**3**: MDVP:Flo(Hz) - Minimum vocal fundamental frequency\n",
    "\n",
    "**4, 5, 6, 7**: MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP -  Several measures of variation in fundamental frequency\n",
    "\n",
    "**8, 9, 10, 11, 12, 13**: MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA -  Several measures of variation in amplitude\n",
    "\n",
    "**14, 15**: NHR,HNR - Two measures of ratio of noise to tonal components in the voice\n",
    "\n",
    "**16, 17**: RPDE,D2 - Two nonlinear dynamical complexity measures\n",
    "\n",
    "**18**: DFA - Signal fractal scaling exponent\n",
    "\n",
    "**19, 20, 21**: spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation\n",
    "\n",
    "**22**: status - Health status of the subject (one) - Parkinson's, (zero) - healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values\n",
    "X_data = data[:, :22] # Take all rows and all columns but the last one\n",
    "y_data = data[:, 22] # Take all rows and only the last column\n",
    "# Split data set into 70% train - 30% test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=0)\n",
    "\n",
    "# Standardize the X_train feature to have a mean of 0 and a standard deviation of 1\n",
    "scaler = StandardScaler() # Create scaler\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train) # Scale feature\n",
    "X_test_scaled = scaler.transform(X_test) # Transform feature matrix\n",
    "num_feature = len(df.columns) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Rescaled X_train array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.20277786,  3.55007322,  1.46378495, ...,  0.37523479,\n",
       "         1.38253798,  0.70950026],\n",
       "       [-0.08490869, -0.40139205,  0.69127853, ..., -0.57555776,\n",
       "        -0.31740397, -0.64646829],\n",
       "       [-1.11208247, -0.68412437, -0.30308418, ..., -0.30690231,\n",
       "        -1.04720163,  1.20993771],\n",
       "       ...,\n",
       "       [ 0.41632981,  2.41567671, -0.86941153, ...,  1.69605993,\n",
       "         1.55536714,  0.84509927],\n",
       "       [ 2.43201619,  0.56163635,  1.59509347, ..., -0.36324299,\n",
       "        -0.93743211, -1.26715615],\n",
       "       [-1.06441596, -0.89097029, -0.36804959, ..., -0.46543789,\n",
       "        -1.3048217 , -0.32328725]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Rescaled X_test array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.35887755, -1.00188268, -0.66104715, ..., -1.83845482,\n",
       "        -0.87167251, -0.28969673],\n",
       "       [-0.41832753, -0.43650645,  0.37155845, ..., -0.94120095,\n",
       "        -0.90410363, -0.70662576],\n",
       "       [ 1.19374787,  0.07167505,  1.93460009, ..., -0.63181426,\n",
       "        -2.21110636, -1.59259592],\n",
       "       ...,\n",
       "       [ 0.80957851,  0.07518158,  0.80306179, ...,  1.31294332,\n",
       "         1.51043619,  1.04630896],\n",
       "       [ 0.3139159 ,  0.01670996, -0.89316488, ...,  1.90191687,\n",
       "         2.45455716,  1.05131043],\n",
       "       [ 0.06226042, -0.14643695,  0.64581462, ...,  1.31198126,\n",
       "         1.24165312,  1.26813762]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Data Pre-processing\n",
    "### 2.5.1 Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape : (136, 22)\n",
      "Projected data shape : (136, 22)\n",
      "Explained variance : 1.0\n"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "projected = pca.fit_transform(X_train_scaled)\n",
    "# print(projected[:, 0])\n",
    "print(\"Training data shape :\", X_train.shape)\n",
    "print(\"Projected data shape :\", projected.shape)\n",
    "print(\"Explained variance :\", np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Calculating the optimal number of PCA components, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "total = sum(pca.explained_variance_)\n",
    "current_sum = 0\n",
    "while(current_sum / total < 0.99):\n",
    "    current_sum += pca.explained_variance_[k]\n",
    "    k += 1\n",
    "print(k)\n",
    "num_feature = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 Data Pre-processing using PCA with optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=k, whiten=True)\n",
    "X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear Logistic Regression\n",
    "This **Linear Logistic Regression** model uses a logistic function to model a binary dependant variable output with a penalty parameter that is $l2-regularised$.\n",
    "\n",
    "We start with **linear logistic regression** as the baseline of classification models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_reg score on training data: 0.8824\n",
      "log_reg score on testing data: 0.8814\n"
     ]
    }
   ],
   "source": [
    "#-------------- \n",
    "# Linear Logistic Regression \n",
    "#--------------\n",
    "# Instantiate and train the machine learning model\n",
    "log_reg = LogisticRegression(solver='lbfgs', penalty='l2')\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "print(\"log_reg score on training data:\", round(log_reg.score(X_train_scaled,y_train), 4))\n",
    "print(\"log_reg score on testing data:\", round(log_reg.score(X_test_scaled,y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_reg score on training data after 20 fold cross-validation\n",
      "Accuracy: 0.8458 (+/- 0.00), AUROC Score: 0.8642 (+/- 0.00)\n",
      "log_reg score on testing data after 20 fold cross-validation\n",
      "Accuracy: 0.8814 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Perform K = 20 fold cross validation and evaluate the performance scores \n",
    "score_ACC = np.mean(cross_val_score(log_reg, X_train_scaled, y_train, scoring = 'accuracy', cv=20))\n",
    "score_AUROC = np.mean(cross_val_score(log_reg, X_train_scaled, y_train, scoring = 'roc_auc', cv=20))\n",
    "score_P = np.mean(cross_val_score(log_reg, X_train_scaled, y_train, scoring = 'precision', cv=20))\n",
    "score_AP = np.mean(cross_val_score(log_reg, X_train_scaled, y_train, scoring = 'average_precision', cv=20))\n",
    "score_F1 = np.mean(cross_val_score(log_reg, X_train_scaled, y_train, scoring = 'f1', cv=20))\n",
    "score_RECALL = np.mean(cross_val_score(log_reg, X_train_scaled, y_train, scoring = 'recall', cv=20))\n",
    "\n",
    "# Print the Accuracy and AUROC performance scores of this model\n",
    "print(\"log_reg score on training data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f), AUROC Score: %.4f (+/- %.2f)\" % (score_ACC.mean(), score_ACC.std(), score_AUROC.mean(), score_AUROC.std() ))\n",
    "\n",
    "# Calculate, save and print the accuracy of the model on the testing data \n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "score_ACC_TEST = (cm[0,0] + cm[1,1] )/len(X_test_scaled)\n",
    "print(\"log_reg score on testing data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f)\" % (score_ACC_TEST.mean(), score_ACC_TEST.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 K-Nearest Neighbours (KNN) Classifier\n",
    "The **k-NN classification** model classifies by using the majority vote of its neighbours with the object being assigned to the class most common among its k-nearest neighbours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn score on training data: 0.9044\n",
      "knn score on testing data: 0.9153\n"
     ]
    }
   ],
   "source": [
    "#-------------- \n",
    "# KNN Classifier\n",
    "#--------------\n",
    "# Instantiate and train the machine learning model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print(\"knn score on training data:\", round(knn.score(X_train_scaled,y_train), 4))\n",
    "print(\"knn score on testing data:\", round(knn.score(X_test_scaled,y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn score on training data after 20 fold cross-validation\n",
      "Accuracy: 0.8449 (+/- 0.00), AUROC Score: 0.8879 (+/- 0.00)\n",
      "knn score on testing data after 20 fold cross-validation\n",
      "Accuracy: 0.9153 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Perform K = 20 fold cross validation and evaluate the performance scores \n",
    "score_ACC = np.mean(cross_val_score(knn, X_train_scaled, y_train, scoring = 'accuracy', cv=20))\n",
    "score_AUROC = np.mean(cross_val_score(knn, X_train_scaled, y_train, scoring = 'roc_auc', cv=20))\n",
    "score_P = np.mean(cross_val_score(knn, X_train_scaled, y_train, scoring = 'precision', cv=20))\n",
    "score_AP = np.mean(cross_val_score(knn, X_train_scaled, y_train, scoring = 'average_precision', cv=20))\n",
    "score_F1 = np.mean(cross_val_score(knn, X_train_scaled, y_train, scoring = 'f1', cv=20))\n",
    "score_RECALL = np.mean(cross_val_score(knn, X_train_scaled, y_train, scoring = 'recall', cv=20))\n",
    "\n",
    "# Print the Accuracy and AUROC performance scores of this model\n",
    "print(\"knn score on training data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f), AUROC Score: %.4f (+/- %.2f)\" % (score_ACC.mean(), score_ACC.std(), score_AUROC.mean(), score_AUROC.std() ))\n",
    "\n",
    "# Calculate, save and print the accuracy of the model on the testing data \n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "score_ACC_TEST = (cm[0,0] + cm[1,1] )/len(X_test_scaled)\n",
    "print(\"knn score on testing data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f)\" % (score_ACC_TEST.mean(), score_ACC_TEST.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.3 Linear Support Vector Machine (SVM)\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. It uses a subset of training points in the decision function (called support vectors) allowing it to be memory efficient. The SVM model is used as it is versatile to be used with kernel functions and be effective in high dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc score on training data: 0.8897\n",
      "svc score on testing data: 0.8475\n"
     ]
    }
   ],
   "source": [
    "#-------------- \n",
    "# Linear SVM \n",
    "#--------------\n",
    "# Instantiate and train the machine learning model\n",
    "svc = SVC(kernel = \"linear\")\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "print(\"svc score on training data:\", round(svc.score(X_train_scaled,y_train), 4))\n",
    "print(\"svc score on testing data:\", round(svc.score(X_test_scaled,y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc score on training data after 20 fold cross-validation\n",
      "Accuracy: 0.8399 (+/- 0.00), AUROC Score: 0.8517 (+/- 0.00)\n",
      "svc score on testing data after 20 fold cross-validation\n",
      "Accuracy: 0.8475 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Perform K = 20 fold cross validation and evaluate the performance scores \n",
    "score_ACC = np.mean(cross_val_score(svc, X_train_scaled, y_train, scoring = 'accuracy', cv=20))\n",
    "score_AUROC = np.mean(cross_val_score(svc, X_train_scaled, y_train, scoring = 'roc_auc', cv=20))\n",
    "score_P = np.mean(cross_val_score(svc, X_train_scaled, y_train, scoring = 'precision', cv=20))\n",
    "score_AP = np.mean(cross_val_score(svc, X_train_scaled, y_train, scoring = 'average_precision', cv=20))\n",
    "score_F1 = np.mean(cross_val_score(svc, X_train_scaled, y_train, scoring = 'f1', cv=20))\n",
    "score_RECALL = np.mean(cross_val_score(svc, X_train_scaled, y_train, scoring = 'recall', cv=20))\n",
    "\n",
    "# Print the Accuracy and AUROC performance scores of this model\n",
    "print(\"svc score on training data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f), AUROC Score: %.4f (+/- %.2f)\" % (score_ACC.mean(), score_ACC.std(), score_AUROC.mean(), score_AUROC.std() ))\n",
    "\n",
    "# Calculate, save and print the accuracy of the model on the testing data \n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "score_ACC_TEST = (cm[0,0] + cm[1,1] )/len(X_test_scaled)\n",
    "print(\"svc score on testing data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f)\" % (score_ACC_TEST.mean(), score_ACC_TEST.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Kernel RBF SVM\n",
    "Based on the results we got from Linear SVM, it seems that the feature space provided was not rich enough to linearly descibe all relationships between classes. Using the fact that SVM is susceptible to the kernel trick, we can use the Kernel Radial Basis Function (RBF) SVM model to raise the feature space to infinite dimensions with the RBF kernel and observe any improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_RBF score on training data: 0.9191\n",
      "svc_RBF score on testing data: 0.9322\n"
     ]
    }
   ],
   "source": [
    "#-------------- \n",
    "# Kernel RBF SVM \n",
    "#--------------\n",
    "# Instantiate and train the machine learning model\n",
    "svc_RBF = SVC(kernel = \"rbf\", gamma = \"auto\")\n",
    "svc_RBF.fit(X_train_scaled, y_train)\n",
    "print(\"svc_RBF score on training data:\", round(svc_RBF.score(X_train_scaled,y_train), 4))\n",
    "print(\"svc_RBF score on testing data:\", round(svc_RBF.score(X_test_scaled,y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_RBF score on training data after 20 fold cross-validation\n",
      "Accuracy: 0.8768 (+/- 0.00), AUROC Score: 0.8617 (+/- 0.00)\n",
      "svc_RBF score on testing data after 20 fold cross-validation\n",
      "Accuracy: 0.9322 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Perform K = 20 fold cross validation and evaluate the performance scores \n",
    "score_ACC = np.mean(cross_val_score(svc_RBF, X_train_scaled, y_train, scoring = 'accuracy', cv=20))\n",
    "score_AUROC = np.mean(cross_val_score(svc_RBF, X_train_scaled, y_train, scoring = 'roc_auc', cv=20))\n",
    "score_P = np.mean(cross_val_score(svc_RBF, X_train_scaled, y_train, scoring = 'precision', cv=20))\n",
    "score_AP = np.mean(cross_val_score(svc_RBF, X_train_scaled, y_train, scoring = 'average_precision', cv=20))\n",
    "score_F1 = np.mean(cross_val_score(svc_RBF, X_train_scaled, y_train, scoring = 'f1', cv=20))\n",
    "score_RECALL = np.mean(cross_val_score(svc_RBF, X_train_scaled, y_train, scoring = 'recall', cv=20))\n",
    "\n",
    "# Print the Accuracy and AUROC performance scores of this model\n",
    "print(\"svc_RBF score on training data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f), AUROC Score: %.4f (+/- %.2f)\" % (score_ACC.mean(), score_ACC.std(), score_AUROC.mean(), score_AUROC.std() ))\n",
    "\n",
    "# Calculate, save and print the accuracy of the model on the testing data \n",
    "y_pred = svc_RBF.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "score_ACC_TEST = (cm[0,0] + cm[1,1] )/len(X_test_scaled)\n",
    "print(\"svc_RBF score on testing data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f)\" % (score_ACC_TEST.mean(), score_ACC_TEST.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Adaptive Gradient Boosting (AdaBoost)\n",
    "The **AdaBoost Classifier** is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "\n",
    "This gradient boosting models such as AdaBoost helps weigh the averages of the different weaker classifier models together and help reduce the possibility of imbalance classes impacting the performance so we expect it to perform slightly better than random forest models. \n",
    "\n",
    "**Ensemble Learning**\n",
    "\n",
    "Boosting ensemble methods creates a strong classifier from a number of weaker classifiers by first building a classification model from the training data and then creating another classification model that attempts to correct the errors of the first model and so on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ada score on training data: 1.0\n",
      "ada score on testing data: 0.9153\n"
     ]
    }
   ],
   "source": [
    "#-------------- \n",
    "# AdaBoost\n",
    "#--------------\n",
    "# Instantiate and train the machine learning model\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train_scaled, y_train)\n",
    "print(\"ada score on training data:\", round(ada.score(X_train_scaled,y_train), 4))\n",
    "print(\"ada score on testing data:\", round(ada.score(X_test_scaled,y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ada score on training data after 20 fold cross-validation\n",
      "Accuracy: 0.7887 (+/- 0.00), AUROC Score: 0.8417 (+/- 0.00)\n",
      "ada score on testing data after 20 fold cross-validation\n",
      "Accuracy: 0.9153 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Perform K = 20 fold cross validation and evaluate the performance scores \n",
    "score_ACC = np.mean(cross_val_score(ada, X_train_scaled, y_train, scoring = 'accuracy', cv=20))\n",
    "score_AUROC = np.mean(cross_val_score(ada, X_train_scaled, y_train, scoring = 'roc_auc', cv=20))\n",
    "score_P = np.mean(cross_val_score(ada, X_train_scaled, y_train, scoring = 'precision', cv=20))\n",
    "score_AP = np.mean(cross_val_score(ada, X_train_scaled, y_train, scoring = 'average_precision', cv=20))\n",
    "score_F1 = np.mean(cross_val_score(ada, X_train_scaled, y_train, scoring = 'f1', cv=20))\n",
    "score_RECALL = np.mean(cross_val_score(ada, X_train_scaled, y_train, scoring = 'recall', cv=20))\n",
    "\n",
    "# Print the Accuracy and AUROC performance scores of this model\n",
    "print(\"ada score on training data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f), AUROC Score: %.4f (+/- %.2f)\" % (score_ACC.mean(), score_ACC.std(), score_AUROC.mean(), score_AUROC.std() ))\n",
    "\n",
    "# Calculate, save and print the accuracy of the model on the testing data \n",
    "y_pred = ada.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "score_ACC_TEST = (cm[0,0] + cm[1,1] )/len(X_test_scaled)\n",
    "print(\"ada score on testing data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f)\" % (score_ACC_TEST.mean(), score_ACC_TEST.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Random Forest\n",
    "**The Random Forest Classifier** is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf score on training data: 1.0\n",
      "rf score on testing data: 0.9322\n"
     ]
    }
   ],
   "source": [
    "#-------------- \n",
    "# Random Forest\n",
    "#--------------\n",
    "# Instantiate and train the machine learning model\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "print(\"rf score on training data:\", round(rf.score(X_train_scaled,y_train), 4))\n",
    "print(\"rf score on testing data:\", round(rf.score(X_test_scaled,y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf score on training data after 20 fold cross-validation\n",
      "Accuracy: 0.8470 (+/- 0.00), AUROC Score: 0.8875 (+/- 0.00)\n",
      "rf score on testing data after 20 fold cross-validation\n",
      "Accuracy: 0.9322 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Perform K = 20 fold cross validation and evaluate the performance scores \n",
    "score_ACC = np.mean(cross_val_score(rf, X_train_scaled, y_train, scoring = 'accuracy', cv=20))\n",
    "score_AUROC = np.mean(cross_val_score(rf, X_train_scaled, y_train, scoring = 'roc_auc', cv=20))\n",
    "score_P = np.mean(cross_val_score(rf, X_train_scaled, y_train, scoring = 'precision', cv=20))\n",
    "score_AP = np.mean(cross_val_score(rf, X_train_scaled, y_train, scoring = 'average_precision', cv=20))\n",
    "score_F1 = np.mean(cross_val_score(rf, X_train_scaled, y_train, scoring = 'f1', cv=20))\n",
    "score_RECALL = np.mean(cross_val_score(rf, X_train_scaled, y_train, scoring = 'recall', cv=20))\n",
    "\n",
    "# Print the Accuracy and AUROC performance scores of this model\n",
    "print(\"rf score on training data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f), AUROC Score: %.4f (+/- %.2f)\" % (score_ACC.mean(), score_ACC.std(), score_AUROC.mean(), score_AUROC.std() ))\n",
    "\n",
    "# Calculate, save and print the accuracy of the model on the testing data \n",
    "y_pred = rf.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "score_ACC_TEST = (cm[0,0] + cm[1,1] )/len(X_test_scaled)\n",
    "print(\"rf score on testing data after 20 fold cross-validation\\nAccuracy: %.4f (+/- %.2f)\" % (score_ACC_TEST.mean(), score_ACC_TEST.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kenyew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kenyew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kenyew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kenyew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kenyew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kenyew/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kenyew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kenyew/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel.fit(X_train_scaled, y_train, epochs=75, batch_size=50, validation_data=(X_test_scaled,y_test))\\nscore=model.evaluate(X_test_scaled,y_test)\\nscore\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=num_feature))\n",
    "model.add(Dense(units=32, activation='sigmoid'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "opti=keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# opti=keras.optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "# opti=keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "model.compile(optimizer=opti, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "model.fit(X_train_scaled, y_train, epochs=75, batch_size=50, validation_data=(X_test_scaled,y_test))\n",
    "score=model.evaluate(X_test_scaled,y_test)\n",
    "score\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation and Results\n",
    "\n",
    "#### **1. Linear Logistic Regression**\n",
    "#### **2. K-Nearest Neighbours**\n",
    "#### **3. Linear SVM**\n",
    "#### **4. Kernel RBF SVM**\n",
    "#### **5. AdaBoost**\n",
    "#### **6. Random Forest**\n",
    "#### **7. Neural Networks**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
